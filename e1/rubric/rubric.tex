\documentclass[11pt,a4paper]{article}

\usepackage{tadoc}

\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{xcolor}
%\usepackage{graphicx}


\usepackage{minted}
\usemintedstyle{autumn}
\setminted{linenos,breaklines,tabsize=4,xleftmargin=1.5em}

\usepackage{dirtree}

%\renewcommand{\multirowsetup}{\centering} 

\usepackage{pdfpages}


\title{VE572 --- Methods and Tools for Big Data}
\subtitle{Midterm Rubric}
\author{\href{mailto:liuyh615@sjtu.edu.cn}{Yihao} and \href{mailto:AuroraZYJ@sjtu.edu.cn}{Yanjun}}
\semester{Summer}
\year{2019}
\blockinfo{
	\begin{center}
		\textbf{This document should be kept secret.}
	\end{center}	
}

% whether or not to display the instructor line
\noinstructor

%\pagenumbering{gobble}

\begin{document}

\maketitle

\section*{Exercise 1 --- Big data or not big data?}
\begin{enumerate}
	\item What is the data size?
	\begin{enumerate}
		\item 1G-10G: Spark without Hadoop
		\item 10G-100G: Mapreduce with Hadoop
		\item 100G+: Spark with Hadoop
	\end{enumerate}
	\item What is the data type?
	\begin{enumerate}
		\item Real time: Spark
		\item Batch: Mapreduce
	\end{enumerate}
	\item What does the company want to do with the data?
	\begin{enumerate}
		\item Search: Drill
		\item Implement algorithms: Spark
	\end{enumerate}
\end{enumerate}


\section*{Exercise 2 --- MapReduce}

\subsection*{1. Determine all the FOFs in the following toy example.}

\inputminted{shell}{../data.txt}

\subsection*{2. Write the Hadoop pseudocode
for the first MapReduce Job. Assume a simple input text file
with a list of names on each line, the user as first field followed by all his friends. For the
output we expect a simple text file where each line is composed of a user and a FOF followed
by the number of friends they have in common.}

\inputminted{java}{../src/main/java/com/ve572/e1/FindFOF.java}

\subsection*{3. Write the Hadoop pseudocode
for the second MapReduce job. Assume the previous output
file as input, and as output a simple text file where each line is composed of a user and all his
FOF ordered with respect to the number of common friends; for each FOF also display the
number of common friends.}


\inputminted{java}{../src/main/java/com/ve572/e1/CountFOF.java}

\section*{Exercise 3 --- Course questions}
\subsection*{1 HDFS}
\subsubsection*{a) What is the default replication level in HDFS?[1]}
The default replication factor is 3.
\subsubsection*{b) Parallel is often seen as more efficient than serial. When writing a file in HDFS, blocks are first sent to a DataNode which forwards them to another, which sends them to another, and so on…Why is this process not parallelised, i.e. send blocks to all the DataNodes at the same time?[4]}
If the blocks are sent to all the DataNodes at the same time, it will be slow due to the limited throughput.[2] Thus when writing a file in HDFS, the blocks are sent to a DataNode at a time. Once the blocks are written in a DataNode, it is no need to forward them to other DataNodes.[2]

\subsubsection*{c) Explain how to find a file in HDFS.[3]}
\begin{enumerate}
	\item Each datanode announces the blocks it has. 
	\item The namenode keeps all the information in its memory. 
	\item When a write occurs an entry is added to the edit log.
\end{enumerate}

\subsubsection*{d) What are the namespace image and edit log?[3]}
Namespace image: Stroing the entire file system namespace,  including the mapping of blocks to files and file system properties.[1.5]\\
Edit log: Recording every change that occurs to file system metadata, such as creating a new file in HDFS and changing the replication factor of a file.[1.5]

\subsubsection*{e) Is it possible to have several NameNodes in a cluster? If so explain how it works.[3]}
It is possible.[0.5] We can use HDFS federation.[0.5] 
\begin{enumerate}
	\item Split the filesystem over several independent namenodes. Each namenode has a namespace and its own pool of blocks [1]
	\item A namespace with a block pool is called namespace volume and a datanode is not attached to a specific namespace volume. [1]
\end{enumerate}

\subsection*{2. YARN}
\subsubsection*{a) Explain how an application is launched and run using YARN.[3]}
\begin{enumerate}
	\item Application client submit a YARN application to Resource Manager.
	\item Resource Manager contacts Node Manager to launch a new container and run Application Master in it.[1]
	\item Application Master asks Resource Manager for allocating the resources.[1]
	\item Application manager gets the resources information from Resource Manager and it launches the container through other Node Manager.[1]
\end{enumerate}

\subsubsection*{b) Would you recommend the fair or capacity scheduler? Explain the when and why.}
Fair scheduling is a method of assigning resources to applications such that all apps get, on average, an equal share of resources over time.[1] It is a good default for small to medium sized clusters[0.5] since it is more flexible and allows for jobs to consume unused resources in the cluster.[1] \\
Capacity scheduler is designed to run Hadoop applications as a shared, multi-tenant cluster in an operator-friendly manner while maximizing the throughput and the utilization of the cluster.[1] It's generally used on large clusters with lots of different workloads with different needs[0.5] since it can give each organization capacity guarantees.[1]

\subsection*{3. Briefly explain the similarities and differences between MapReduce and Spark. [7]}
Similarity: Both are highly scalable and can be used in a cluster.[1] 
MapReduce: MapReduce takes two stages to process data, Map and Reduce. It reads and writes from disk and thus slower and has high latency.[1] It uses replication for fault tolerance, which significantly increase the completion times for operations with a single failure.[2]\\
Spark: Spark is lightning fast cluster computing tool. It is much faster than MapReduce and has low-latency computing.[1] It uses RDDs and DAG for fault tolerance. If an RDD is lost, it is easy to recompute a new one by using the original transformations.[2]


\subsection*{4. Drill}
\subsubsection*{a) What is Zookeeper, and why is it a core component of Drill’s strategy?[2]}
ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.[1]\\
Drill uses ZooKeeper to maintain cluster membership and health-check information.[1]

\subsubsection*{b) Explain what is a Drillbit and how it functions.[3]}
A Drillbit is the process running on each active Drill node that coordinates, plans, and executes queries, as well as distributes query work across the cluster to maximize data locality.[1]

The Drillbit receives the query from a client. A SQL parser in the DrillBit parses the SQL and form a logical plan.[0.5]

The Drillbit sends the logical plan into a optimizer to optimize and convert the logical plan into a physical plan that describes how to execute the query.[0.5]

A parallelizer in the Drillbit transforms the physical plan into multiple phases, called major and minor fragments. [0.5]

These fragments create a multi-level execution tree that rewrites the query and executes it in parallel against the configured data sources, sending the results back to the client or application.[0.5]


\end{document}

